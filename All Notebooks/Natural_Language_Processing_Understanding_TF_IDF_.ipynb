{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing  Understanding TF - IDF .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5sXvZZHycvaf2gBLBRyLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TusharNautiyal-web/CollabNotebooks/blob/main/All%20Notebooks/Natural_Language_Processing_Understanding_TF_IDF_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Natural Language Processing \n",
        "1. Tokenization\n",
        "2. Stemming\n",
        "3. Lematization\n",
        "4. Term Frequency and Inverse Document Frequency"
      ],
      "metadata": {
        "id": "VpNIp2Dwho4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is TF - IDF ? ‚è¨\n",
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents.\n",
        "\n",
        "**Formula of TF - IDF**\n",
        "\n",
        "TF = No of repeated Words in a sentence/ No of words in a sentence\n",
        "\n",
        "IDF = No of sentences / No of sentence containing word\n",
        "```\n",
        "TF = No of repeated words in a sentence\n",
        "    ----------------------------------\n",
        "    Total no of words in a sentence\n",
        "\n",
        "      No of sentences\n",
        "IDF = ------------------------------\n",
        "      No of sentence containing word\n",
        "```\n",
        "\n",
        "After this do **TF*IDF** multiplying them will give us our result which will be carrying our word importance. \n",
        "\n",
        "**Advantage**\n",
        "1. Intutive\n",
        "2. Bit difficult then Bag of words\n",
        "3. Words Importance are getting captured\n",
        "\n",
        "**Disadvantage**\n",
        "1. Sparsity\n",
        "2. Out of Volcabulary Problems"
      ],
      "metadata": {
        "id": "_dhTIH-U9LgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 Assing a Paragraph\n",
        "Paragraph is our corpus and we will be giving this corpus to tokenize and reduce it to important words.\n",
        "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.\n"
      ],
      "metadata": {
        "id": "vSJdoYDTkXX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Sentence From Geek For Geeks\n",
        "paragraph = '''\n",
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. \n",
        "Text preprocessing includes both Stemming as well as Lemmatization. \n",
        "Many times people find these two terms confusing. \n",
        "Some treat these two as the same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n",
        "'''"
      ],
      "metadata": {
        "id": "aPKEEMWDhp4Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 NLTK\n",
        "We are using NLTK Library to do Stemmming and tokeninzation. Lets Get Started."
      ],
      "metadata": {
        "id": "75KCNncQknrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer # For Stemming\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "H1zhtH_khsFl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punkt has the feature tokenize so to use tokenization we need to download punk using below statement**\n",
        "\n",
        "***Also Download These Files***"
      ],
      "metadata": {
        "id": "tQQoj7sNkxl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87d0Qq6iL_M",
        "outputId": "248b8c67-2142-4c8d-d148-f621c174e5c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "cGnwr6KtiZPr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAutWxnLijfQ",
        "outputId": "94fb1319-8feb-4d5c-d0bd-a884a0687c7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.', 'Lemmatization is similar to stemming but it brings context to the words.', 'So it links words with similar meanings to one word.', 'Text preprocessing includes both Stemming as well as Lemmatization.', 'Many times people find these two terms confusing.', 'Some treat these two as the same.', 'Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        "Now After doing tokenization we can either choose stemming or lematizing as we wish \n",
        "Stemming will not save the meaning of the words given to it whereas lematizer will now its upto you as per your usecase. But before lets do some pre-processing of our sentence \n",
        "\n",
        "To choose Lemmatizer or stemmer we use below code."
      ],
      "metadata": {
        "id": "74WWPZeCk8v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "uFmAs5q4ilNd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Stemming ?**\n",
        "\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "6CRHcSojj687"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem('thinking'))\n",
        "print(stemmer.stem('history'))\n",
        "print(stemmer.stem('going'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9QPhakljo6m",
        "outputId": "82ba0b73-3396-4213-bc68-46de0b57a002"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "think\n",
            "histori\n",
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets remove Special Characters and only take the words. For this we can use regular expression re.sub which will subtract everything other then a-z, A-Z"
      ],
      "metadata": {
        "id": "DOAT5ZChl1RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "iFgK_WBVjybG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRNP_lgZmmA_",
        "outputId": "2aecac9e-20b7-418a-a3f2-eafc09301d31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lemmatization process grouping together different inflected form word analyzed single item',\n",
              " 'lemmatization similar stemming brings context word',\n",
              " 'link word similar meaning one word',\n",
              " 'text preprocessing includes stemming well lemmatization',\n",
              " 'many time people find two term confusing',\n",
              " 'treat two',\n",
              " 'actually lemmatization preferred stemming lemmatization morphological analysis word']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets Apply Stemming\n",
        "1. We will take all sentences from corpus and tokenize them (Converting Sentence into Words)\n",
        "2. We will after that apply stemming on each word of tokenized sentence and also remove all stopwords.\n"
      ],
      "metadata": {
        "id": "g5aGo0dMm5ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting All Sentences\n",
        "l1 = [] # This will take all words.\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    # because we are reading english paragraph and stopwords can be duplicate so to not have any duplicates\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      l1.append(stemmer.stem(word))\n",
        "      print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQionCVhmwTy",
        "outputId": "fed96635-1fc3-4e28-8aee-f3a64588358a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmat\n",
            "process\n",
            "group\n",
            "togeth\n",
            "differ\n",
            "inflect\n",
            "form\n",
            "word\n",
            "analyz\n",
            "singl\n",
            "item\n",
            "lemmat\n",
            "similar\n",
            "stem\n",
            "bring\n",
            "context\n",
            "word\n",
            "link\n",
            "word\n",
            "similar\n",
            "mean\n",
            "one\n",
            "word\n",
            "text\n",
            "preprocess\n",
            "includ\n",
            "stem\n",
            "well\n",
            "lemmat\n",
            "mani\n",
            "time\n",
            "peopl\n",
            "find\n",
            "two\n",
            "term\n",
            "confus\n",
            "treat\n",
            "two\n",
            "actual\n",
            "lemmat\n",
            "prefer\n",
            "stem\n",
            "lemmat\n",
            "morpholog\n",
            "analysi\n",
            "word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALso lets use Lemmatizer as we are trying to understand how Natural Language Processing is done.\n",
        "l2 = [] # This will contain Lematized words\n",
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      ans = lemmatizer.lemmatize(word)\n",
        "      l2.append(ans)\n"
      ],
      "metadata": {
        "id": "FkVv3GmxoRaH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3v2Dpj0oe6_",
        "outputId": "c2a8a961-60bd-49d1-ea8c-bddf1b4af557"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lemmatization',\n",
              " 'process',\n",
              " 'grouping',\n",
              " 'together',\n",
              " 'different',\n",
              " 'inflected',\n",
              " 'form',\n",
              " 'word',\n",
              " 'analyzed',\n",
              " 'single',\n",
              " 'item',\n",
              " 'lemmatization',\n",
              " 'similar',\n",
              " 'stemming',\n",
              " 'brings',\n",
              " 'context',\n",
              " 'word',\n",
              " 'link',\n",
              " 'word',\n",
              " 'similar',\n",
              " 'meaning',\n",
              " 'one',\n",
              " 'word',\n",
              " 'text',\n",
              " 'preprocessing',\n",
              " 'includes',\n",
              " 'stemming',\n",
              " 'well',\n",
              " 'lemmatization',\n",
              " 'many',\n",
              " 'time',\n",
              " 'people',\n",
              " 'find',\n",
              " 'two',\n",
              " 'term',\n",
              " 'confusing',\n",
              " 'treat',\n",
              " 'two',\n",
              " 'actually',\n",
              " 'lemmatization',\n",
              " 'preferred',\n",
              " 'stemming',\n",
              " 'lemmatization',\n",
              " 'morphological',\n",
              " 'analysis',\n",
              " 'word']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Features \n",
        "\n",
        "Out of all This Data We will use bag of words technique. Below code is what u need to do. We will use Tfidvectorizer to create features out of words.\n",
        "max_features = 3 means if its only present in 3 times then it will get used."
      ],
      "metadata": {
        "id": "W104hkhDp4ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer(ngram_range = (1,1), max_features = 3)"
      ],
      "metadata": {
        "id": "4Z6z4L6Ppzt-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = cv.fit_transform(corpus)\n",
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDOsDijxrH6y",
        "outputId": "53d38253-2442-4841-aef1-e00180d28204"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lemmatization': 0, 'stemming': 1, 'word': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "count_matrix = X.toarray()\n",
        "df = pd.DataFrame(data=count_matrix,columns = cv.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1pKh55Sqhm3",
        "outputId": "afcc7da8-e889-41e7-d2f9-c728667346c0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8i8TJ_AmyviW",
        "outputId": "91d2cbd0-dc01-43a0-cab6-411b1db22890"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   lemmatization  stemming      word\n",
              "0       0.707107  0.000000  0.707107\n",
              "1       0.548273  0.631500  0.548273\n",
              "2       0.000000  0.000000  1.000000\n",
              "3       0.655595  0.755113  0.000000\n",
              "4       0.000000  0.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11f525d6-1730-4b8b-b897-0fa108c80cbe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemmatization</th>\n",
              "      <th>stemming</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.548273</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.548273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.655595</td>\n",
              "      <td>0.755113</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11f525d6-1730-4b8b-b897-0fa108c80cbe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11f525d6-1730-4b8b-b897-0fa108c80cbe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11f525d6-1730-4b8b-b897-0fa108c80cbe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV5BeJWqywuO",
        "outputId": "70e2beb1-a272-42de-9196-a9686467f626"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lemmatization', 'stemming', 'word'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**That's All For NLP TF-IDF**\n",
        "\n",
        "Which is a technique to convert text into features using Tfidfvectorizer. If you like such content subscribe to my youtube\n",
        "\n",
        "Youtube-Link: <a href = 'https://www.youtube.com/channel/UCsfYqXa3LoaLkB-9F2vmplA'>Click Here</a>\n",
        "\n",
        "Or Connect with me on linkedin.\n",
        "\n",
        "Linkedin: <a href = 'https://www.linkedin.com/in/tusharnautiyal/'>Click Here </a>\n",
        "\n",
        "@ author: Tushar Nautiyal\n",
        "Hope so you liked it üòÄ"
      ],
      "metadata": {
        "id": "0LTFmX1czBz2"
      }
    }
  ]
}